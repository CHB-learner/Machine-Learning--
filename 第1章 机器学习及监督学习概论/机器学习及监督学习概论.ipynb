{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第1章 统计学习方法概论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行分析与预测的一门学科。\n",
    "统计学习包括监督学习、非监督学习、半监督学习和强化学习。\n",
    "\n",
    "2．统计学习方法三要素——模型、策略、算法，对理解统计学习方法起到提纲挈领的作用。\n",
    "\n",
    "3．本书主要讨论监督学习，监督学习可以概括如下：从给定有限的训练数据出发， 假设数据是独立同分布的，而且假设模型属于某个假设空间，应用某一评价准则，从假设空间中选取一个最优的模型，使它对已给训练数据及未知测试数据在给定评价标准意义下有最准确的预测。\n",
    "\n",
    "4．统计学习中，进行模型选择或者说提高学习的泛化能力是一个重要问题。如果只考虑减少训练误差，就可能产生过拟合现象。模型选择的方法有正则化与交叉验证。学习方法泛化能力的分析是统计学习理论研究的重要课题。\n",
    "\n",
    "5．分类问题、标注问题和回归问题都是监督学习的重要问题。本书中介绍的统计学习方法包括感知机、$k$近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场。这些方法是主要的分类、标注以及回归方法。它们又可以归类为生成方法与判别方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最小二乘法拟和曲线\n",
    "\n",
    "高斯于1823年在误差$e_1,…,e_n$独立同分布的假定下,证明了最小二乘方法的一个最优性质: 在所有无偏的线性估计类中,最小二乘方法是其中方差最小的！\n",
    "对于数据$(x_i, y_i)   (i=1, 2, 3...,m)$\n",
    "\n",
    "拟合出函数$h(x)$\n",
    "\n",
    "有误差，即残差：$r_i=h(x_i)-y_i$\n",
    "\n",
    "此时$L2$范数(残差平方和)最小时，$h(x)$ 和 $y$ 相似度最高，更拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的$H(x)$为$n$次的多项式，$H(x)=w_0+w_1x+w_2x^2+...w_nx^n$\n",
    "\n",
    "$w(w_0,w_1,w_2,...,w_n)$为参数\n",
    "\n",
    "最小二乘法就是要找到一组 $w(w_0,w_1,w_2,...,w_n)$ ，使得$\\sum_{i=1}^n(h(x_i)-y_i)^2$ (残差平方和) 最小\n",
    "\n",
    "即，求 $min\\sum_{i=1}^n(h(x_i)-y_i)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题1.1\n",
    "&emsp;&emsp;说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型$n$次独立的数据生成结果，其中$k$次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。\n",
    "\n",
    "**解答：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "伯努利模型的极大似然估计以及贝叶斯估计中的**统计学习方法三要素**如下：  \n",
    "1. **极大似然估计**  \n",
    "**模型：** $\\mathcal{F}=\\{f|f_p(x)=p^x(1-p)^{(1-x)}\\}$  \n",
    "**策略：** 最大化似然函数  \n",
    "**算法：** $\\displaystyle \\mathop{\\arg\\min}_{p} L(p)= \\mathop{\\arg\\min}_{p} \\binom{n}{k}p^k(1-p)^{(n-k)}$\n",
    "2. **贝叶斯估计**  \n",
    "**模型：** $\\mathcal{F}=\\{f|f_p(x)=p^x(1-p)^{(1-x)}\\}$  \n",
    "**策略：** 求参数期望  \n",
    "**算法：**\n",
    "$$\\begin{aligned}  E_\\pi\\big[p \\big| y_1,\\cdots,y_n\\big]\n",
    "& = {\\int_0^1}p\\pi (p|y_1,\\cdots,y_n) dp \\\\\n",
    "& = {\\int_0^1} p\\frac{f_D(y_1,\\cdots,y_n|p)\\pi(p)}{\\int_{\\Omega}f_D(y_1,\\cdots,y_n|p)\\pi(p)dp}dp \\\\\n",
    "& = {\\int_0^1}\\frac{p^{k+1}(1-p)^{(n-k)}}{\\int_0^1 p^k(1-p)^{(n-k)}dp}dp\n",
    "\\end{aligned}$$\n",
    "\n",
    "**伯努利模型的极大似然估计：**  \n",
    "定义$P(Y=1)$概率为$p$，可得似然函数为：$$L(p)=f_D(y_1,y_2,\\cdots,y_n|\\theta)=\\binom{n}{k}p^k(1-p)^{(n-k)}$$方程两边同时对$p$求导，则：$$\\begin{aligned}\n",
    "0 & = \\binom{n}{k}[kp^{k-1}(1-p)^{(n-k)}-(n-k)p^k(1-p)^{(n-k-1)}]\\\\\n",
    "& = \\binom{n}{k}[p^{(k-1)}(1-p)^{(n-k-1)}(m-kp)]\n",
    "\\end{aligned}$$可解出$p$的值为$p=0,p=1,p=k/n$，显然$\\displaystyle P(Y=1)=p=\\frac{k}{n}$  \n",
    "\n",
    "**伯努利模型的贝叶斯估计：**  \n",
    "定义$P(Y=1)$概率为$p$，$p$在$[0,1]$之间的取值是等概率的，因此先验概率密度函数$\\pi(p) = 1$，可得似然函数为： $$L(p)=f_D(y_1,y_2,\\cdots,y_n|\\theta)=\\binom{n}{k}p^k(1-p)^{(n-k)}$$  \n",
    "根据似然函数和先验概率密度函数，可以求解$p$的条件概率密度函数：$$\\begin{aligned}\\pi(p|y_1,\\cdots,y_n)&=\\frac{f_D(y_1,\\cdots,y_n|p)\\pi(p)}{\\int_{\\Omega}f_D(y_1,\\cdots,y_n|p)\\pi(p)dp}\\\\\n",
    "&=\\frac{p^k(1-p)^{(n-k)}}{\\int_0^1p^k(1-p)^{(n-k)}dp}\\\\\n",
    "&=\\frac{p^k(1-p)^{(n-k)}}{B(k+1,n-k+1)}\n",
    "\\end{aligned}$$所以$p$的期望为：$$\\begin{aligned}\n",
    "E_\\pi[p|y_1,\\cdots,y_n]&={\\int}p\\pi(p|y_1,\\cdots,y_n)dp \\\\\n",
    "& = {\\int_0^1}\\frac{p^{(k+1)}(1-p)^{(n-k)}}{B(k+1,n-k+1)}dp \\\\\n",
    "& = \\frac{B(k+2,n-k+1)}{B(k+1,n-k+1)}\\\\\n",
    "& = \\frac{k+1}{n+2}\n",
    "\\end{aligned}$$\n",
    "$\\therefore \\displaystyle P(Y=1)=\\frac{k+1}{n+2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题1.2\n",
    "&emsp;&emsp;通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "假设模型的条件概率分布是$P_{\\theta}(Y|X)$，现推导当损失函数是对数损失函数时，极大似然估计等价于经验风险最小化。\n",
    "极大似然估计的似然函数为：$$L(\\theta)=\\prod_D P_{\\theta}(Y|X)$$两边取对数：$$\\ln L(\\theta) = \\sum_D \\ln P_{\\theta}(Y|X) \\\\ \n",
    "\\mathop{\\arg \\max}_{\\theta} \\sum_D \\ln P_{\\theta}(Y|X) = \\mathop{\\arg \\min}_{\\theta} \\sum_D (- \\ln P_{\\theta}(Y|X))$$ \n",
    "反之，经验风险最小化等价于极大似然估计，亦可通过经验风险最小化推导极大似然估计。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
